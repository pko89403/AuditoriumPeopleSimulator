{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GMF_with_TF.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1zgRhzmjMasGPiB89kdChA_AW6BJeOsUk",
      "authorship_tag": "ABX9TyPnG84z4L2OrrCe6sQAzbpG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pko89403/AuditoriumPeopleSimulator/blob/master/GMF_with_TF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k20bCc9MdL09"
      },
      "source": [
        "# GMF 를 Tensorflow로 구현 \n",
        "\n",
        "## 사용하는 데이터 \n",
        "- lastfm 데이터셋 \n",
        "## Evaluation\n",
        "- top@K ( K = 10 )\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvN3-btgeQxF",
        "outputId": "927ff9df-c1ea-43f4-867c-73f82c6ff545",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "! ls \"/content/drive/My Drive/data\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "aclImdb      index.html\t\t  lastfm-dataset-360K.tar.gz\n",
            "imdb.tar.gz  lastfm-dataset-360K\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-cX-Jl1c_WR",
        "outputId": "139d635b-b4cf-4b0a-c2de-07ff661385bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "!wget \"http://mtg.upf.edu/static/datasets/last.fm/lastfm-dataset-360K.tar.gz\" -P \"/content/drive/My Drive/data/\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-27 02:09:52--  http://mtg.upf.edu/static/datasets/last.fm/lastfm-dataset-360K.tar.gz\n",
            "Resolving mtg.upf.edu (mtg.upf.edu)... 84.89.139.55\n",
            "Connecting to mtg.upf.edu (mtg.upf.edu)|84.89.139.55|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 569202935 (543M) [application/x-gzip]\n",
            "Saving to: ‘/content/drive/My Drive/data/lastfm-dataset-360K.tar.gz.1’\n",
            "\n",
            "lastfm-dataset-360K 100%[===================>] 542.83M  5.48MB/s    in 1m 43s  \n",
            "\n",
            "2020-10-27 02:11:35 (5.27 MB/s) - ‘/content/drive/My Drive/data/lastfm-dataset-360K.tar.gz.1’ saved [569202935/569202935]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHlTSllZeTuz",
        "outputId": "f9185760-6e8e-4f40-87f6-1f6a7acef746",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "!tar -zxvf \"/content/drive/My Drive/data/lastfm-dataset-360K.tar.gz\" -C \"/content/drive/My Drive/data/lastfm/\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tar: /content/drive/My Drive/data/lastfm: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NfgxyWnjHCx",
        "outputId": "27a4580f-7d27-4454-9bd2-307ac1c206c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        }
      },
      "source": [
        "!head \"/content/drive/My Drive/data/lastfm-dataset-360K/usersha1-artmbid-artname-plays.tsv\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "00000c289a1829a808ac09c00daf10bc3c4e223b\t3bd73256-3905-4f3a-97e2-8b341527f805\tbetty blowtorch\t2137\n",
            "00000c289a1829a808ac09c00daf10bc3c4e223b\tf2fb0ff0-5679-42ec-a55c-15109ce6e320\tdie Ärzte\t1099\n",
            "00000c289a1829a808ac09c00daf10bc3c4e223b\tb3ae82c2-e60b-4551-a76d-6620f1b456aa\tmelissa etheridge\t897\n",
            "00000c289a1829a808ac09c00daf10bc3c4e223b\t3d6bbeb7-f90e-4d10-b440-e153c0d10b53\telvenking\t717\n",
            "00000c289a1829a808ac09c00daf10bc3c4e223b\tbbd2ffd7-17f4-4506-8572-c1ea58c3f9a8\tjuliette & the licks\t706\n",
            "00000c289a1829a808ac09c00daf10bc3c4e223b\t8bfac288-ccc5-448d-9573-c33ea2aa5c30\tred hot chili peppers\t691\n",
            "00000c289a1829a808ac09c00daf10bc3c4e223b\t6531c8b1-76ea-4141-b270-eb1ac5b41375\tmagica\t545\n",
            "00000c289a1829a808ac09c00daf10bc3c4e223b\t21f3573f-10cf-44b3-aeaa-26cccd8448b5\tthe black dahlia murder\t507\n",
            "00000c289a1829a808ac09c00daf10bc3c4e223b\tc5db90c4-580d-4f33-b364-fbaa5a3a58b5\tthe murmurs\t424\n",
            "00000c289a1829a808ac09c00daf10bc3c4e223b\t0639533a-0402-40ba-b6e0-18b067198b73\tlunachicks\t403\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeI6qAidfTrR"
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "import pickle"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5TW00QS0kAd"
      },
      "source": [
        "def load_dataset():\n",
        "  \n",
        "  \n",
        "  df = pd.read_csv(\"/content/drive/My Drive/data/lastfm-dataset-360K/usersha1-artmbid-artname-plays.tsv\", sep='\\t')\n",
        "  \n",
        "\n",
        "  df = df.drop(df.columns[1], axis=1)\n",
        "  df.columns = ['user', 'item', 'plays']\n",
        "  df = df.dropna()\n",
        "  df = df.loc[df.plays != 0]\n",
        "\n",
        "\n",
        "  df_count = df.groupby(['user']).count()\n",
        "  df['count'] = df.groupby('user')['user'].transform('count')\n",
        "  df = df[df['count'] > 1]\n",
        "\n",
        "  \n",
        "  # Return Series of codes as well as the index.\n",
        "  df['user_id'] = df['user'].astype('category').cat.codes\n",
        "  df['item_id'] = df['item'].astype('category').cat.codes  \n",
        "\n",
        "\n",
        "  item_lookup = df[['item_id', 'item']].drop_duplicates()\n",
        "  item_lookup['item_id'] = item_lookup.item_id.astype(str)\n",
        "\n",
        "\n",
        "  user_lookup = df[['user_id', 'user']].drop_duplicates()\n",
        "  user_lookup['user_id'] = user_lookup.user_id.astype(str)\n",
        "\n",
        "\n",
        "  df = df[['user_id', 'item_id', 'plays']]\n",
        "  users = list(np.sort(df.user_id.unique()))\n",
        "  items = list(np.sort(df.item_id.unique()))\n",
        "\n",
        "\n",
        "  df_train, df_test = train_test_split(df)\n",
        "\n",
        "\n",
        "  rows = df_train.user_id.astype(int)\n",
        "  cols = df_train.item_id.astype(int)\n",
        "\n",
        "  values = list(df_train.plays)\n",
        "\n",
        "  uids = np.array(rows.tolist())\n",
        "  iids = np.array(cols.tolist())\n",
        "\n",
        "\n",
        "  df_neg = get_negatives(uids, iids, items, df_test)\n",
        "\n",
        "  return uids, iids, df_train, df_test, df_neg, users, items, item_lookup"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IW589VqmvB8"
      },
      "source": [
        "def mask_first(x):\n",
        "  result = np.ones_like(x)\n",
        "  result[0] = 0\n",
        "  return result"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5NGfR1qd1xl"
      },
      "source": [
        "def train_test_split(df):\n",
        "  df_test = df.copy(deep=True)\n",
        "  df_train = df.copy(deep=True)\n",
        "\n",
        "  # Group by user and select only the first item for each user \n",
        "  df_test = df_test.groupby(['user_id']).first()\n",
        "  df_test['user_id'] = df_test.index\n",
        "  df_test = df_test[['user_id', 'item_id', 'plays']]\n",
        "  print(df_test.index.name)\n",
        "  df_test.rename(index={'name': ''}, inplace=True)\n",
        "\n",
        "  # Remove the same items as we for our test set in our training set\n",
        "  mask = df.groupby(['user_id'])['user_id'].transform(mask_first).astype(bool)\n",
        "  df_train = df.loc[mask]\n",
        "\n",
        "  return df_train, df_test"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLvChBZ3n67b"
      },
      "source": [
        "def get_negatives(uids, iids, items, df_test):\n",
        "  negativeList = []\n",
        "  test_u = df_test['user_id'].values.tolist()\n",
        "  test_i = df_test['item_id'].values.tolist()\n",
        "\n",
        "  test_ratings = list(zip(test_u, test_i))\n",
        "  zipped = set(zip(uids, iids))\n",
        "\n",
        "  for (u, i) in test_ratings:\n",
        "    negatives = []\n",
        "    negatives.append((u, i))\n",
        "    for t in range(100):\n",
        "      j = np.random.randint(len(items)) # Get random item id\n",
        "      while (u, j) in zipped: # Check if there is an interaction\n",
        "        j = np.random.randint(len(items)) # If yes, generate a new item id \n",
        "      negatives.append(j) # Once a negative interaction is found we add it\n",
        "    negativeList.append(negatives)\n",
        "\n",
        "  df_neg = pd.DataFrame(negativeList)\n",
        "\n",
        "  return df_neg"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c386jd_qqTd1"
      },
      "source": [
        "def get_train_instances():\n",
        "  user_input, item_input, labels = [], [], []\n",
        "  zipped = set(zip(uids, iids))\n",
        "\n",
        "  for (u, i) in zip(uids, iids):\n",
        "    # Add positive interaction\n",
        "    user_input.append(u)\n",
        "    item_input.append(i)\n",
        "    labels.append(1)\n",
        "\n",
        "    # Sample random negative interaction\n",
        "    for t in range(num_neg):\n",
        "      j = np.random.randint(len(items))\n",
        "      while (u, i) in zipped:\n",
        "        j = np.random.randint(len(items))\n",
        "      \n",
        "      user_input.append(u)\n",
        "      item_input.append(j)\n",
        "      labels.append(0)\n",
        "\n",
        "    return user_input, item_input, labels"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWdg5RRysvRd"
      },
      "source": [
        "def random_mini_batches(U, I, L, mini_batch_size=256):\n",
        "  mini_batches = []\n",
        "  \n",
        "  shuffled_U, shuffled_I, shuffled_L = shuffle(U, I, L)\n",
        "\n",
        "  num_complete_batches = int(math.floor(len(U)/mini_batch_size))\n",
        "  for k in range(0, num_complete_batches):\n",
        "    mini_batch_U = shuffled_U[k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
        "    mini_batch_I = shuffled_I[k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
        "    mini_batch_L = shuffled_L[k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
        "\n",
        "    mini_batch = (mini_batch_U, mini_batch_I, mini_batch_L)\n",
        "    mini_batches.append(mini_batch)\n",
        "  \n",
        "  if len(U) % mini_batch_size != 0:\n",
        "    mini_batch_U = shuffled_U[num_complete_batches * mini_batch_size : len(U)]\n",
        "    mini_batch_I = shuffled_I[num_complete_batches * mini_batch_size : len(U)]\n",
        "    mini_batch_L = shuffled_L[num_complete_batches * mini_batch_size : len(U)]\n",
        "\n",
        "    mini_batch = (mini_batch_U, mini_batch_I, mini_batch_L)\n",
        "    mini_batches.append(mini_batch)\n",
        "\n",
        "  return mini_batches "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6Dwmd1svTJf"
      },
      "source": [
        "def get_hits(k_ranked, holdout):\n",
        "  for item in h_ranked:\n",
        "    if item == holdout:\n",
        "      return 1\n",
        "    return 0"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NA44Fe7vgb_"
      },
      "source": [
        "def eval_rating(idx, test_ratings, test_negatives, K):\n",
        "  map_item_score = {}\n",
        "\n",
        "  # get the negative interactions our userr\n",
        "  items = test_negatives[idx]\n",
        "  # Get the user idx\n",
        "  user_idx = test_ratings[idx][0]\n",
        "  # Get the item idx -> holdout item\n",
        "  holdout = test_ratings[idx][1]\n",
        "\n",
        "  # Add the holdout to the end of the negative interactions list.\n",
        "  items.append(holdout)\n",
        "\n",
        "  # Prepare our user and item arrays for tensorflow\n",
        "  predict_user = np.full(len(items), user_idx, dtype='int32').reshape(-1,1)\n",
        "  np_items = np.array(items).reshape(-1, 1)\n",
        "\n",
        "  # Feed user and items into the TF graph.\n",
        "  predictions = session.run([output_layer], feed_dict={user: predict_user, item: np_items})\n",
        "\n",
        "  # Get the predicted score to item id \n",
        "  for i in range(len(items)):\n",
        "    current_item = items[i]\n",
        "    map_item_score[current_item] = predictions[i]\n",
        "  \n",
        "  # Get the K highest ranked items as a list\n",
        "  h_ranked = heapq.nlargest(K, map_item_score, key=map_item_score.get)\n",
        "\n",
        "  # Get a list of hit or no hit.\n",
        "  hits = get_hits(h_ranked, holdout)\n",
        "\n",
        "  return hits \n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NjRbAW2ykQU"
      },
      "source": [
        "def evaluate(df_neg, K=10):\n",
        "  hits = []\n",
        "\n",
        "  test_u = df_test['user_id'].values.tolist()\n",
        "  test_i = df_test['item_id'].values.tolist()\n",
        "\n",
        "  test_ratings = list(zip(test_u, test_i))\n",
        "\n",
        "  df_neg = df_neg.drop(df_neg.columns[0], axis=1)\n",
        "  test_negatives = df_neg.values.tolist()\n",
        "\n",
        "  for idx in range(len(test_ratings)):\n",
        "    hitrate = eval_rating(idx, test_ratings, test_negatives, K)\n",
        "    hits.append(hitrate)\n",
        "\n",
        "  return hits"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgOSF45Ez9cU"
      },
      "source": [
        "import tensorflow as tf \n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import math\n",
        "import heapq\n",
        "from tqdm import tqdm "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCZKNn4v0T9i",
        "outputId": "f0dd705c-6632-4702-ec0e-640bcba3459f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Load and prepare our data\n",
        "uids, iids, df_train, df_test, df_neg, users, items, item_lookup = load_dataset()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "user_id\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RO8NRWxMcbuu"
      },
      "source": [
        "# Hyper-Parameters\n",
        "num_neg = 4\n",
        "latent_features = 8\n",
        "epochs = 10\n",
        "batch_size = 256\n",
        "learning_rate = 1e-3"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8CPScQt0aIG"
      },
      "source": [
        "# The MLP model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFDFbDkTcn0r"
      },
      "source": [
        "class MLP(tf.keras.Model):\n",
        "  def __init__(self, user_size, item_size):\n",
        "    super(MLP, self).__init__(name='')\n",
        "\n",
        "    # User Embedding\n",
        "    self.u_var = tf.keras.layers.Embedding(input_dim=user_size,\n",
        "                                      output_dim=32,\n",
        "                                      embeddings_initializer='uniform')\n",
        "    self.u_flatten = tf.keras.layers.Flatten()\n",
        "\n",
        "    # Item Embedding\n",
        "    self.i_var = tf.keras.layers.Embedding(input_dim=item_size,\n",
        "                                      output_dim=32,\n",
        "                                      embeddings_initializer='uniform')\n",
        "    self.i_flatten = tf.keras.layers.Flatten()\n",
        "\n",
        "    # Concatenate our two Embedding vectors togehter\n",
        "    self.concatenated = tf.keras.layers.Concatenate(axis=1)\n",
        "    self.dropout = tf.keras.layers.Dropout(0.2)\n",
        "\n",
        "    # Below we add our four hidden layers along with batch\n",
        "    # Normalization and Dropout. We use relu as the Activation Funtion\n",
        "    self.layer_1 = tf.keras.layers.Dense(64, activation='relu', name='layer1')\n",
        "    self.batch_norm1 = tf.keras.layers.BatchNormalization(name='batch_norm1')\n",
        "    self.dropout1 = tf.keras.layers.Dropout(0.2, name='dropout1')\n",
        "\n",
        "    self.layer_2 = tf.keras.layers.Dense(32, activation='relu', name='layer2')\n",
        "    self.batch_norm2 = tf.keras.layers.BatchNormalization(name='batch_norm2')\n",
        "    self.dropout2 = tf.keras.layers.Dropout(0.2, name='dropout2')\n",
        "\n",
        "    self.layer_3 = tf.keras.layers.Dense(16, activation='relu', name='layer3')\n",
        "    self.layer_4 = tf.keras.layers.Dense(8, activation='relu', name='layer4')\n",
        "\n",
        "    # Our final single neuron output layer\n",
        "    self.output_layer = tf.keras.layers.Dense(1,\n",
        "      kernel_initializer=\"lecun_uniform\",\n",
        "      name='output_layer')\n",
        "\n",
        "\n",
        "  def call(self, userId, itemId, training=False):\n",
        "    user = self.u_var(userId)\n",
        "    user = self.u_flatten(user)\n",
        "\n",
        "    item = self.i_var(itemId)\n",
        "    item = self.i_flatten(item)\n",
        "\n",
        "    concat_ui = self.concatenated([user, item])\n",
        "    drop_concat = self.dropout(concat_ui)\n",
        "\n",
        "    x = self.layer_1(drop_concat)\n",
        "    x = self.batch_norm1(x, training=training)\n",
        "    x = self.dropout1(x)\n",
        "    x = self.layer_2(drop_concat)\n",
        "    x = self.batch_norm2(x, training=training)\n",
        "    x = self.dropout2(x)\n",
        "    x = self.layer_3(x)\n",
        "    x = self.layer_4(x)\n",
        "\n",
        "    out = self.output_layer(x)\n",
        "    return out"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzoXhmLOjZf1"
      },
      "source": [
        "model_MLP = MLP(user_size=len(users),\n",
        "                item_size=len(items))"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cXwMjeUJHyn",
        "outputId": "a0c4330c-b40a-4f73-94c1-d2f4715d4bcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 655
        }
      },
      "source": [
        "model_MLP.summary()\n",
        "#model_MLP.trainable_variables\n",
        "#model_MLP.trainable_weights"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"mlp_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_12 (Embedding)     multiple                  11482656  \n",
            "_________________________________________________________________\n",
            "flatten_12 (Flatten)         multiple                  0         \n",
            "_________________________________________________________________\n",
            "embedding_13 (Embedding)     multiple                  9355392   \n",
            "_________________________________________________________________\n",
            "flatten_13 (Flatten)         multiple                  0         \n",
            "_________________________________________________________________\n",
            "concatenate_4 (Concatenate)  multiple                  0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          multiple                  0         \n",
            "_________________________________________________________________\n",
            "layer1 (Dense)               multiple                  4160      \n",
            "_________________________________________________________________\n",
            "batch_norm1 (BatchNormalizat multiple                  256       \n",
            "_________________________________________________________________\n",
            "dropout1 (Dropout)           multiple                  0         \n",
            "_________________________________________________________________\n",
            "layer2 (Dense)               multiple                  2080      \n",
            "_________________________________________________________________\n",
            "batch_norm2 (BatchNormalizat multiple                  128       \n",
            "_________________________________________________________________\n",
            "dropout2 (Dropout)           multiple                  0         \n",
            "_________________________________________________________________\n",
            "layer3 (Dense)               multiple                  528       \n",
            "_________________________________________________________________\n",
            "layer4 (Dense)               multiple                  136       \n",
            "_________________________________________________________________\n",
            "output_layer (Dense)         multiple                  9         \n",
            "=================================================================\n",
            "Total params: 20,845,345\n",
            "Trainable params: 20,845,153\n",
            "Non-trainable params: 192\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrZnCGV5CXzA",
        "outputId": "798613e9-c7d5-48a9-bbcd-4c92582caedb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model_MLP(userId = np.array([1]), itemId = np.array([1]))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.01099623]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-PHy38XIf_K"
      },
      "source": [
        "# Define Loss Func & Gradient Func"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vDuAzyVE9H4"
      },
      "source": [
        "def loss(model, user, item, label):\n",
        "  labels = tf.cast(label, tf.float32)\n",
        "  logits = tf.cast(model(user, item), tf.float32)\n",
        "  loss_object = tf.nn.sigmoid_cross_entropy_with_logits(labels = labels,\n",
        "                                                        logits = logits)\n",
        "  return loss_object\n",
        "  "
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnPl4c5qHoVw",
        "outputId": "2c241e9a-42b0-4167-f26b-980d095ac42a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "l = loss(model_MLP, np.array([1]), np.array([1]), np.ones((1,1)))\n",
        "print(f\"Test Loss : {l}\")"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([[1.]], shape=(1, 1), dtype=float32) tf.Tensor([[0.01099623]], shape=(1, 1), dtype=float32)\n",
            "Test Loss : [[0.6876642]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "An3Z0gFBIynb"
      },
      "source": [
        "def grad(model, user, item, label):\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss_value = loss(model, user, item, label)\n",
        "  return loss_value, tape.gradient(loss_value, model.trainable_variables)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjNsAPlZJfpN"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWB55j2aKBYm"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "\n",
        "  # get training input.\n",
        "  user_inputs, item_inputs, labels = get_train_instances()  \n",
        "\n",
        "  # generate a list of mini-batches\n",
        "  minibatches = random_mini_batches(user_inputs, item_inputs, labels)\n",
        "\n",
        "  epoch_loss_avg = tf.keras.metrics.Mean()\n",
        "\n",
        "\n",
        "  progress = tqdm(total=len(minibatches))\n",
        "  \n",
        "  for minibatch in minibatches:\n",
        "    user_input = np.array(minibatch[0]).reshape(-1, 1)\n",
        "    item_input = np.array(minibatch[1]).reshape(-1, 1)\n",
        "    label = np.array(minibatch[2]).reshape(-1, 1)\n",
        "    # Optimizing model\n",
        "    loss_value, grads = grad(model_MLP, user_input, item_input, label)\n",
        "    optimizer.apply_gradient(zip(grads, model_MLP.trainable_variables))\n",
        "\n",
        "    # Tracking Progress Status\n",
        "    epoch_loss_avg(loss_value) # Append Current Batch Loss\n",
        "\n",
        "    # Update the progress \n",
        "    progress.update(1)\n",
        "    progress.set_description('Epoch : %d - Loss : %.3f' % (epoch+1, l))\n",
        "  progress.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwf-i6-n1Fb_"
      },
      "source": [
        "# The GMF Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeM7GjGNjsX1"
      },
      "source": [
        ""
      ]
    }
  ]
}